import numpy as np
import jax.numpy as jnp
import haiku as hk
import jax
import networkx as nx
import optax

from collections import namedtuple
from jax.scipy.stats import norm
from functools import partial

from gflownet_sl.gflownet import GFlowNetState


@hk.without_apply_rng
@hk.transform
def model(inputs, parents):
    # Following the architecture from DiBS
    # https://arxiv.org/pdf/2105.11839.pdf (Section 6.3)
    outputs = hk.nets.MLP(
        (5, 1),
        activation=jax.nn.relu,
        with_bias=True,
        activate_final=False,
        name='mlp'
    )(inputs * parents)
    return jnp.squeeze(outputs, axis=-1)


NormalParameters = namedtuple('NormalParameters', ['loc', 'scale'])


class NonLinearGaussian:
    def __init__(self, optimizer, model=model, obs_noise=0.1):
        self.optimizer = optimizer
        self.model = model
        self.obs_noise = obs_noise

    def sample_ground_truth_parameters(self, key, adjacency):
        num_variables = adjacency.shape[0]

        # Sample (unmasked) parameters
        subkeys = jax.random.split(key, num_variables)
        inputs = jnp.zeros((num_variables, 1, num_variables))
        params = jax.vmap(self.model.init)(subkeys, inputs, adjacency.T)

        # Mask the weights of the first layer
        weights = params['mlp/~/linear_0']['w']
        weights = weights * jnp.expand_dims(adjacency.T, axis=2)

        return hk.data_structures.merge(params, {'mlp/~/linear_0': {'w': weights}})

    def sample_data(self, key, params, adjacency, num_samples):
        num_variables = adjacency.shape[0]
        samples = jnp.zeros((num_samples, num_variables))
        subkeys = jax.random.split(key, num_variables)

        # Ancestral sampling
        graph = nx.from_numpy_array(adjacency, create_using=nx.DiGraph)
        for node, subkey in zip(nx.topological_sort(graph), subkeys):
            # Forward pass of the MLP to get the mean of the observations
            node_params = jax.tree_util.tree_map(lambda param: param[node], params)
            node_mean = self.model.apply(node_params, samples, adjacency[:, node])

            # Sample from a Normal distribution
            epsilon = jax.random.normal(subkey, shape=node_mean.shape)
            node_samples = node_mean + self.obs_noise * epsilon
            samples = samples.at[:, node].set(node_samples)

        return np.asarray(samples)

    def sample_thetas(self, key, params, num_samples):
        leaves, treedef = jax.tree_util.tree_flatten(params.loc)
        subkeys = jax.random.split(key, len(leaves))
        epsilons = [jax.random.normal(subkey, (num_samples,) + leaf.shape)
            for (subkey, leaf) in zip(subkeys, leaves)]
        epsilons = jax.tree_util.tree_unflatten(treedef, epsilons)
        return jax.tree_util.tree_map(
            lambda loc, scale, epsilon: loc + scale * epsilon,
            params.loc, params.scale, epsilons)

    def log_likelihood(self, parents, theta, data, y):
        mean = self.model.apply(theta, data, parents)
        log_likelihoods = norm.logpdf(y, loc=mean, scale=self.obs_noise)
        return jnp.sum(log_likelihoods, axis=0)

    @staticmethod
    def _partition_first_layer_weights(tree):
        first_weights, other_params = hk.data_structures.partition(
            lambda module_name, name, _: (module_name == 'mlp/~/linear_0') and (name == 'w'),
            tree
        )
        return (first_weights['mlp/~/linear_0']['w'], other_params)

    def kl_divergence(self, parents, first_weights, other_params):
        def _kl(loc, scale):
            # From https://arxiv.org/abs/1312.6114 (Appendix B)
            return -0.5 * (1 + 2 * jnp.log(scale) - (loc ** 2) - (scale ** 2))

        # Compute the KL-divergence for the weights of the first layer.
        # Mask the components, depending on the adjacency matrix
        kls_first = _kl(*first_weights)
        kls_first = jnp.sum(kls_first.T * parents)

        # Compute the KL-divergence for all other parameters
        kls_other = jax.tree_util.tree_map(_kl, *other_params)
        kls_other = jax.tree_util.tree_reduce(lambda x, y: x + jnp.sum(y), kls_other, 0.)

        return kls_first + kls_other

    def delta_score(self, params, key, adjacencies, actions, data, num_samples_thetas):
        num_variables = adjacencies.shape[1]
        v_log_likelihood = jax.vmap(self.log_likelihood, in_axes=(None, 0, None, None))  # vmapping over thetas

        # Sample the parameters thetas of the conditional probabilities for the MC estimate
        thetas = self.sample_thetas(key, params, num_samples_thetas)
        # Partition the model parameters into the weights of the first layer, and the rest
        first_weights, other_params = map(lambda args: NormalParameters(*args),
            zip(*map(NonLinearGaussian._partition_first_layer_weights, params)))

        def _delta_score(adjacency, action, thetas, first_weights, other_params, data):
            # We are reusing the same parameters thetas for estimating both
            # expectations for the local score before (on G) and after (on G') adding the edge.
            source_node, target_node = jnp.divmod(action, num_variables)

            # Select the parents, data & parameters for the target node
            parents, y = adjacency[:, target_node], data[:, target_node]
            thetas = jax.tree_util.tree_map(lambda param: param[:, target_node], thetas)
            first_weights = jax.tree_util.tree_map(lambda param: param[target_node], first_weights)
            other_params = jax.tree_util.tree_map(lambda param: param[target_node], other_params)

            # Compute the local score for the target variable before adding the edge
            expected_log_likelihood = jnp.mean(v_log_likelihood(parents, thetas, data, y))
            kl_divergence = self.kl_divergence(parents, first_weights, other_params)
            local_score_before = expected_log_likelihood - kl_divergence

            # Compute the local score for the target variable after adding the edge
            parents_after = parents.at[source_node].set(1.)
            expected_log_likelihood = jnp.mean(v_log_likelihood(parents_after, thetas, data, y))
            kl_divergence = self.kl_divergence(parents_after, first_weights, other_params)
            local_score_after = expected_log_likelihood - kl_divergence

            return local_score_after - local_score_before

        delta_scores = jax.vmap(_delta_score, in_axes=(0, 0) + (None,) * 4)(
            adjacencies, actions, thetas, first_weights, other_params, data)
        return jnp.expand_dims(delta_scores, axis=1)

    @partial(jax.jit, static_argnums=(0, 6))
    def compute_delta_score(self, params, state, adjacencies, actions, data, num_samples_thetas):
        key, subkey = jax.random.split(state.key)
        state = state._replace(key=key)
        delta_scores = self.delta_score(
            params, subkey, adjacencies, actions, data, num_samples_thetas)
        return (delta_scores, state)

    def loss(self, params, key, adjacencies, data, num_samples_thetas):
        # if action is specified, compute delta score for specific adjacency and the 
        # next adjacency that follows after the action
        # Sample parameters thetas for the MC estimate
        thetas = self.sample_thetas(key, params, num_samples_thetas)

        # Compute the log-likelihood log P(D | G, theta)
        v_log_likelihood = jax.vmap(self.log_likelihood, in_axes=(1, 0, None, 1))  # vmapping over variables
        v_log_likelihood = jax.vmap(v_log_likelihood, in_axes=(None, 0, None, None))  # vmapping over thetas
        v_log_likelihood = jax.vmap(v_log_likelihood, in_axes=(0, None, None, None))  # vmapping over graphs
        log_likelihoods = v_log_likelihood(adjacencies, thetas, data, data)
        log_likelihoods = jnp.sum(log_likelihoods, axis=2)  # sum over variables

        # Compute KL(q(theta | G) || P(theta | G)), where P(theta | G) = N(0, I)
        # Partition the model parameters into the weights of the first layer, and the rest
        first_weights, other_params = map(lambda args: NormalParameters(*args),
            zip(*map(NonLinearGaussian._partition_first_layer_weights, params)))

        v_kl_divergence = jax.vmap(self.kl_divergence, in_axes=(1, 0, 0))  # vmapping over variables
        v_kl_divergence = jax.vmap(v_kl_divergence, in_axes=(0, None, None))  # vmapping over graphs

        kl_divergences = v_kl_divergence(adjacencies, first_weights, other_params)
        kl_divergences = jnp.sum(kl_divergences, axis=1)

        # The loss is the negative ELBO. TODO: Assume uniform P(G)
        expected_log_likelihood = jnp.mean(log_likelihoods, axis=1)  # Expectation over theta
        return -jnp.mean(expected_log_likelihood - kl_divergences)  # Expectation over graphs

    @partial(jax.jit, static_argnums=(0, 5))
    def step(self, params, state, adjacencies, data, num_samples_thetas):
        key, subkey = jax.random.split(state.key)
        grads = jax.grad(self.loss)(params, subkey, adjacencies, data, num_samples_thetas)

        # Update the params
        updates, state_opt = self.optimizer.update(grads, state.optimizer, params)
        params = optax.apply_updates(params, updates)
        state = GFlowNetState(optimizer=state_opt, key=key)

        return (params, state)

    def init(self, num_variables, key, loc=0., scale=1.):
        # Sample dummy model parameters
        subkeys = jnp.zeros((num_variables, 2), dtype=jnp.uint32)  # Dummy random keys
        inputs = jnp.zeros((num_variables, 1, num_variables))
        adjacency = jnp.zeros((num_variables, num_variables))
        dummy = jax.vmap(self.model.init)(subkeys, inputs, adjacency)
        params = NormalParameters(
            loc=jax.tree_util.tree_map(
                lambda param: jnp.full_like(param, loc), dummy),
            scale=jax.tree_util.tree_map(
                lambda param: jnp.full_like(param, scale), dummy),
        )
        state = GFlowNetState(optimizer=self.optimizer.init(params), key=key)
        return (params, state)
